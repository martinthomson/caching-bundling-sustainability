<!DOCTYPE html>
<meta charset="utf-8">
<title>Sustainability of Bundling and Caching</title>
<script class="remove" src="https://www.w3.org/Tools/respec/respec-w3c">
</script>
<script class="remove">
var respecConfig = {
	specStatus: "draft-finding",
	editors: [
		{
			name: "Sangwhan Moon",
			url: "",
			company: "Google",
		},
		{
			name: "Lea Verou",
			url: "https://lea.verou.me",
			company: "W3C Invited Experts"
		},
		{
			name: "Yves Lafon",
			url: "",
			company: "W3C"
		}
	],
	group: "tag",
	wgPublicList: "www-tag",
	github: "w3ctag/caching-bundling-sustainability",
	format: "markdown",
	localBiblio: {
		"EWP" : {
			title: "Ethical Web Principles",
			href: "https://www.w3.org/2001/tag/doc/ethical-web-principles/",
			publisher: "W3C",
		},
	},
};
</script>
<body class="informative">

<section id="abstract">

This finding examines the sustainability of bundling
and double-keyed caching in web development.
Bundling dependencies into a single file can improve efficiency,
but it results in repeated downloads of resources,
negatively impacting end-user experience.
Double-keyed caching partitions the cache by origin,
increasing storage, energy consumption, and bandwidth.
The finding suggests experimenting with smart delays
for cache fetches and bundling with delta transport as potential solutions.
The finding highlights the need for awareness
of the hidden costs and sustainability risks
and the benefits of reusing client-side resources.

</section>

<section id="sotd">
This document is an early draft and does not reflect the consensus of the TAG.
</section>

## Introduction

Improvements in both the performance and efficiency of modern computers,
combined with tremendous advances in optimizations
within web browsers have made it possible to run
large web applications while providing
a near-native user experience and performance.
This allowed developers to essentially move towards
a development pattern that enables code re-use from many small packages.

The downside of the agility gained by this pattern is that
when this practice was first introduced,
transporting many small files was not efficient,
and modules were not readily available.
Bundling all of these dependencies into a single file
and compressing it became a common practice
for both transport efficiency and easier dependency management,
analogous to static linking in native development.
The unfortunate side effect of this practice is that
it results in the user having to download duplicate code,
as the browser is unable to determine
if a given bundle contains a specific library.

Additionally, security research has unearthed the potential of
fingerprinting users [through resource loading times](https://github.com/w3c/webappsec-subresource-integrity/issues/22);
and the mitigation for that introduced a mechanism
commonly known as double-keyed caching.
This further amplifies the inefficiencies of transport.

The end-user impact is unnecessary repeated downloads of resources
which can mean delay, energy consumption, storage bloat, and data cost.

## Transport Sustainability on the Web Platform

We want to increase broader community awareness
of the hidden costs and sustainability risks
introduced by bundling and double-keyed caching.
Neither transport nor computation is free;
these are costs that both the content provider and the user ends up paying.
Also, different user populations
pay different comparative costs for transport.
Having to download chunks of code that are largely duplicated
repeatedly is detrimental to the end-user experience.

A simple thought experiment would be to think about
how many times jQuery has been downloaded since its introduction;
and how much we could have saved users in data packet fees
if there was some level of re-use on the client side.

## Problems Introduced by Double-keyed Caching

Double-keyed caching was [introduced in 2013](https://bugs.webkit.org/show_bug.cgi?id=110269) to mitigate timing attacks,
allowing malicious code to detect whether certain resources were cached.
The solution that was chosen was to partition the cache by origin,
and re-download resources even if a different origin already cached them.
This meant that the longstanding practice of different websites
sharing resources by linking to a CDN, was no longer effective.
Furthermore, many developers are still unaware of double-keyed caching,
and operate under the impression that loading a commonly requested resource
from a CDN leads to performance improvements.

While double-keyed caching does solve the privacy vulnerability
it was designed to address the negative impact to users is quite substantial.
Loading resources separately for each origin results in:
- Additional storage space to store these cached resources.
- Additional packages are transferred over the wire,
  which, in many scenarios, can be an actual financial burden for users.
- Additional energy is consumed as extra CPU cycles are used for decoding, parsing, and executing resources.
- Additional bandwidth that content users, intermediaries, and providers need to pay.

## The Hidden Cost of Bundling

Bundling lowers latency and gives coherence to the whole package,
but the hidden cost is the unused payload
and the recurring cost of it during an update.
There is also the inherent risk of “write amplification”,
where even a single byte change in a web application requires
downloading the entire application from scratch.
There is a definite cost associated with this,
both from the end of providing the application,
as well as receiving the application.

## Angles for Experimentation

We cannot compromise on the security and privacy guarantees that double keying provides.
Additionally, no immediate replacements provide equivalent conveniences and functionality
that bundling provides as of the time of writing.

If we move the ecosystem to be unbundled, the cost of transport will go up.
However, we cannot simply remove double keying to alleviate that cost.
One potential way forward could be to experiment with mechanisms that
emulate the effects of double-keyed caching through a software layer
but not necessarily trigger the wire transport.

One potential avenue could involve a noising layer on top of a single-keyed cache,
which adds enough noise to prevent attacks that rely on timing (e.g., artificial delays)
or metadata (e.g., adding noise in latent state).

## Call to Action

Unfortunately, the solution to this is largely an unsolved problem
and the web platform does not yet have all the machinery needed to solve this problem.
We would like to see experts from the community
to experiment with different approaches to find a more sustainable solution
and eventually propose a solution that reduces the costs
while preserving the security and privacy guarantees we have today.
